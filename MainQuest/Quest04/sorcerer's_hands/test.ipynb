{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import sys # 오류 처리 시 필요\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 전역 설정 및 모델, MediaPipe, 웹캠 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from './models/model_v1.h5'\n",
      "MediaPipe Hands model initialized for two hands.\n",
      "웹캠이 성공적으로 초기화되었습니다.\n",
      "웹캠 해상도: 640x480\n",
      "Defined actions for prediction: ['flower', 'crown', 'heart_beat', 'firework', 'bear', 'cat', 'son_celebration', 'heart_on_the_cheek', 'gun', 'pipe', 'tiger', 'landmarks']\n",
      "Sequence length for model input: 30\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Global Setup and Initialization\n",
    "\n",
    "# --- 2.1 학습된 모델 로드 ---\n",
    "model_path = './models/model_v1.h5' # 실제 모델 파일 경로\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Model loaded successfully from '{model_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from '{model_path}': {e}\")\n",
    "    print(\"모델 파일이 없거나 경로가 잘못되었습니다. 경로를 확인해주세요.\")\n",
    "    model = None # 모델 로드 실패 시 None으로 설정하여 이후 예측 단계에서 에러 방지\n",
    "\n",
    "\n",
    "# --- 2.2 MediaPipe Hands 모델 초기화 (두 손 감지 설정) ---\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2, # 두 손 감지 설정\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "print(\"MediaPipe Hands model initialized for two hands.\")\n",
    "\n",
    "\n",
    "# --- 2.3 웹캠 초기화 ---\n",
    "cap = cv2.VideoCapture(0) # 기본 웹캠 (대부분 0번)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"\\n--- 오류: 웹캠을 열 수 없습니다! ---\")\n",
    "    print(\"   웹캠이 연결되어 있는지, 드라이버가 올바른지, 다른 프로그램에서 사용 중이 아닌지 확인해주세요.\")\n",
    "    print(\"   이 오류가 발생하면 이후 실시간 추론 셀은 작동하지 않습니다.\")\n",
    "\n",
    "else:\n",
    "    print(\"웹캠이 성공적으로 초기화되었습니다.\")\n",
    "    # 웹캠 해상도 설정\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    print(f\"웹캠 해상도: {int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n",
    "\n",
    "\n",
    "# --- 2.4 제스처 이름 및 시퀀스 길이 설정 ---\n",
    "# 학습 시 사용했던 actions 리스트와 seq_length 값이 정확히 일치해야 합니다!\n",
    "actions = [\n",
    "    'flower',\n",
    "    'crown',\n",
    "    'heart_beat',\n",
    "    'firework',\n",
    "    'bear',\n",
    "    'cat',\n",
    "    'son_celebration',\n",
    "    'heart_on_the_cheek',\n",
    "    'gun',\n",
    "    'pipe',\n",
    "    'tiger',\n",
    "    'landmarks'\n",
    "]\n",
    "seq_length = 30 # 학습 시 사용한 시퀀스 길이와 동일해야 합니다.\n",
    "\n",
    "print(f\"Defined actions for prediction: {actions}\")\n",
    "print(f\"Sequence length for model input: {seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 실시간 제스처 추론 메인 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "flower_gif = Image.open('./img/sunflower.gif')\n",
    "\n",
    "# 모든 프레임 추출\n",
    "flower_frames = []\n",
    "try:\n",
    "    while True:\n",
    "        frame = flower_gif.convert('RGBA')\n",
    "        frame_np = np.array(frame)\n",
    "        frame_cv = cv2.cvtColor(frame_np, cv2.COLOR_RGBA2BGRA)\n",
    "        flower_frames.append(np.array(frame_cv))\n",
    "        flower_gif.seek(flower_gif.tell() + 1)\n",
    "except EOFError:\n",
    "    pass\n",
    "num_flower_frames = len(flower_frames)\n",
    "frame_idx = 0\n",
    "\n",
    "def effect_flower(image, result, frame_idx):\n",
    "    if result.multi_hand_landmarks is None or len(result.multi_hand_landmarks) < 2:\n",
    "        return image\n",
    "    \n",
    "    wrist1 = result.multi_hand_landmarks[0].landmark[0]\n",
    "    wrist2 = result.multi_hand_landmarks[1].landmark[0]\n",
    "\n",
    "    wrist1_x = int(wrist1.x * image.shape[1])\n",
    "    wrist2_x = int(wrist2.x * image.shape[1])\n",
    "    wrist1_y = int(wrist1.y * image.shape[0])\n",
    "\n",
    "    hand1 = result.multi_hand_landmarks[0].landmark[9]\n",
    "    hand2 = result.multi_hand_landmarks[1].landmark[9]\n",
    "\n",
    "    hand1_x = int(hand1.x * image.shape[1])\n",
    "    hand2_x = int(hand2.x * image.shape[1])\n",
    "    hand1_y = int(hand1.y * image.shape[0])\n",
    "    hand2_y = int(hand2.y * image.shape[0])\n",
    "\n",
    "    w1 = max(hand1_x, hand2_x) - min(hand1_x, hand2_x)\n",
    "    w2 = max(hand1_y, hand2_y) - min(hand1_y, hand2_y)\n",
    "\n",
    "    w = max(w1, w2)\n",
    "    x = min(wrist1_x, wrist2_x)\n",
    "    y = wrist1_y\n",
    "\n",
    "    if w == 0:\n",
    "        return image\n",
    "\n",
    "    overlay_img = flower_frames[frame_idx % num_flower_frames]\n",
    "    overlay_img = cv2.resize(overlay_img, (w, int(w * overlay_img.shape[0] / overlay_img.shape[1])))\n",
    "\n",
    "    h = overlay_img.shape[0]\n",
    "\n",
    "    # 이미지 경계 넘어가지 않도록 crop 처리\n",
    "    y1 = max(0, y - (h//2))\n",
    "    y2 = min(y1 + h, image.shape[0])\n",
    "    x1 = max(0, x - (w//2))\n",
    "    x2 = min(image.shape[1], x1 + w)\n",
    "\n",
    "    overlay_crop = overlay_img[0:(y2 - y1), 0:(x2 - x1), :] # overlay_img[(h - (y2 - y1)):, (x1 - x):(x2 - x), :]\n",
    "    mask_crop = overlay_crop[:, :, 3] / 255\n",
    "\n",
    "    for c in range(3):\n",
    "        image[y1:y2, x1:x2, c] = \\\n",
    "            (overlay_crop[:, :, c] * mask_crop) + (image[y1:y2, x1:x2, c] * (1 - mask_crop))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_crown(image, result):   # 프레임 이미지, 적용 좌표, 높이 너비, 효과 이미지\n",
    "    crown = cv2.imread('./img/crown.png', cv2.IMREAD_UNCHANGED)\n",
    "    crown_ratio = crown.shape[0] / crown.shape[1]\n",
    "    if result.multi_hand_landmarks is None or len(result.multi_hand_landmarks) < 2:\n",
    "        return image\n",
    "    hand1 = result.multi_hand_landmarks[0].landmark[0]\n",
    "    hand2 = result.multi_hand_landmarks[1].landmark[0]\n",
    "\n",
    "    hand1_x = int(hand1.x * image.shape[1])\n",
    "    hand2_x = int(hand2.x * image.shape[1])\n",
    "    hand1_y = int(hand1.y * image.shape[0])\n",
    "\n",
    "    w = max(hand1_x, hand2_x) - min(hand1_x, hand2_x)\n",
    "    x = min(hand1_x, hand2_x)\n",
    "    y = hand1_y\n",
    "\n",
    "    if w == 0:\n",
    "        return image # 두 손이 같은 위치면 crown 그리지 않음\n",
    "    \n",
    "    overlay_img = crown.copy()\n",
    "    overlay_img = cv2.resize(overlay_img, (w, int(w*crown_ratio)))\n",
    "\n",
    "    # alpha = overlay_img[:, :, 3]\n",
    "    # mask = alpha / 255\n",
    "\n",
    "    h = overlay_img.shape[0]\n",
    "\n",
    "    # 이미지 경계 넘는 부분 잘라내기\n",
    "    y1 = max(0, y - h)\n",
    "    y2 = y\n",
    "    x1_crop = max(0, x)\n",
    "    x2_crop = min(image.shape[1], x + w)\n",
    "\n",
    "    overlay_crop = overlay_img[(h - (y2 - y1)):, (x1_crop - x):(x2_crop - x), :]\n",
    "    mask_crop = overlay_crop[:, :, 3] / 255\n",
    "\n",
    "    for c in range(3):\n",
    "        image[y1:y2, x1_crop:x2_crop, c] = \\\n",
    "            (overlay_crop[:, :, c] * mask_crop) + (image[y1:y2, x1_crop:x2_crop, c] * (1 - mask_crop))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_gif = Image.open('./img/heartbeat.gif')\n",
    "\n",
    "# 모든 프레임 추출\n",
    "hb_frames = []\n",
    "try:\n",
    "    while True:\n",
    "        frame = hb_gif.convert('RGBA')\n",
    "        frame_np = np.array(frame)\n",
    "        frame_cv = cv2.cvtColor(frame_np, cv2.COLOR_RGBA2BGRA)\n",
    "        hb_frames.append(np.array(frame_cv))\n",
    "        hb_gif.seek(hb_gif.tell() + 1)\n",
    "except EOFError:\n",
    "    pass\n",
    "num_hb_frames = len(hb_frames)\n",
    "hb_frame_idx = 0\n",
    "\n",
    "def effect_heartbeat(image, result, hb_frame_idx):\n",
    "    if result.multi_hand_landmarks is None or len(result.multi_hand_landmarks) < 2:\n",
    "        return image\n",
    "    \n",
    "    wrist1 = result.multi_hand_landmarks[0].landmark[0]\n",
    "    wrist2 = result.multi_hand_landmarks[1].landmark[0]\n",
    "    hand_idx = 0 if wrist1.x < wrist2.x else 1\n",
    "\n",
    "    hand = result.multi_hand_landmarks[hand_idx].landmark[9]\n",
    "    thumb = result.multi_hand_landmarks[hand_idx].landmark[4]\n",
    "    pinky = result.multi_hand_landmarks[hand_idx].landmark[20]\n",
    "\n",
    "    thumb_x = int(thumb.x * image.shape[1])\n",
    "    pinky_x = int(pinky.x * image.shape[1])\n",
    "\n",
    "    x = int(hand.x * image.shape[1])\n",
    "    y = int(pinky.y * image.shape[0]) - 30\n",
    "    w = (max(thumb_x, pinky_x) - min(thumb_x, pinky_x)) * 2\n",
    "\n",
    "    if w == 0:\n",
    "        return image\n",
    "\n",
    "    overlay_img = hb_frames[hb_frame_idx % num_hb_frames]\n",
    "    overlay_img = cv2.resize(overlay_img, (w, int(w * overlay_img.shape[0] / overlay_img.shape[1])))\n",
    "\n",
    "    h = overlay_img.shape[0]\n",
    "\n",
    "    # 이미지 경계 넘어가지 않도록 crop 처리\n",
    "    y1 = max(0, y - (h//2))\n",
    "    y2 = min(y1 + h, image.shape[0])\n",
    "    x1 = max(0, x - (w//2))\n",
    "    x2 = min(image.shape[1], x1 + w)\n",
    "\n",
    "    overlay_crop = overlay_img[0:(y2 - y1), 0:(x2 - x1), :]\n",
    "    mask_crop = overlay_crop[:, :, 3] / 255\n",
    "\n",
    "    for c in range(3):\n",
    "        image[y1:y2, x1:x2, c] = \\\n",
    "            (overlay_crop[:, :, c] * mask_crop) + (image[y1:y2, x1:x2, c] * (1 - mask_crop))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.8.20)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "def effect_tiger(img, result):\n",
    "    if result.multi_hand_landmarks is None or len(result.multi_hand_landmarks) < 2:\n",
    "        return img\n",
    "    \n",
    "    tigerpaw_img = cv2.imread('./img/tigerpaw.png', cv2.IMREAD_UNCHANGED)\n",
    "    if tigerpaw_img is None:\n",
    "        print(\"tigerpaw.png 이미지를 불러올 수 없습니다.\")\n",
    "        exit()\n",
    "\n",
    "    pygame.mixer.init()\n",
    "    tiger_sound = pygame.mixer.Sound('./sound/tiger.mp3')\n",
    "\n",
    "    for idx in range(2):\n",
    "        hand_landmarks = result.multi_hand_landmarks[idx].landmark\n",
    "        # 손바닥 중심 좌표 계산\n",
    "        palm_indices = [0, 5, 9, 13, 17]\n",
    "        x = int(np.mean([hand_landmarks[i].x for i in palm_indices]) * img.shape[1])\n",
    "        y = int(np.mean([hand_landmarks[i].y for i in palm_indices]) * img.shape[0])\n",
    "        # 손 오므림 정도 계산 (중지 끝과 손바닥 중심 거리)\n",
    "        x_f = int(hand_landmarks[8].x * img.shape[1])\n",
    "        y_f = int(hand_landmarks[8].y * img.shape[0])\n",
    "        openness = np.sqrt((x - x_f) ** 2 + (y - y_f) ** 2)\n",
    "        min_size, max_size = 80, 180\n",
    "        size = int(np.clip(openness * 1.8, min_size, max_size))\n",
    "        tigerpaw_resized = cv2.resize(tigerpaw_img, (size, size))\n",
    "        x1, y1 = int(x - size // 2), int(y - size // 2)\n",
    "        # PNG 오버레이\n",
    "        h, w = tigerpaw_resized.shape[:2]\n",
    "        if x1 >= 0 and y1 >= 0 and x1 + w <= img.shape[1] and y1 + h <= img.shape[0]:\n",
    "            if tigerpaw_resized.shape[2] == 4:\n",
    "                alpha_fg = tigerpaw_resized[:, :, 3] / 255.0\n",
    "                alpha_bg = 1.0 - alpha_fg\n",
    "                for c in range(3):\n",
    "                    img[y1:y1+h, x1:x1+w, c] = (alpha_fg * tigerpaw_resized[:, :, c] +\n",
    "                                                alpha_bg * img[y1:y1+h, x1:x1+w, c])\n",
    "            else:\n",
    "                img[y1:y1+h, x1:x1+w] = tigerpaw_resized\n",
    "    # 사운드 한 번만 재생\n",
    "    if not hasattr(effect_tiger, \"played\") or not effect_tiger.played:\n",
    "        tiger_sound.play()\n",
    "        effect_tiger.played = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "WIDTH, HEIGHT = 600, 400\n",
    "GRAVITY = 0.15 # 중력을 약간 높여 더 역동적인 효과\n",
    "TRIGGER_DELAY_SECONDS = 3\n",
    "\n",
    "# MediaPipe Pose 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, x, y, color):\n",
    "        self.x, self.y, self.color = x, y, color\n",
    "        self.lifespan = 255\n",
    "        angle = random.uniform(0, 2 * math.pi)\n",
    "        speed = random.uniform(1, 8) # 조금 더 강하게 폭발\n",
    "        self.vx = math.cos(angle) * speed\n",
    "        self.vy = math.sin(angle) * speed\n",
    "        \n",
    "    def update(self):\n",
    "        self.lifespan -= 5\n",
    "        if self.lifespan < 0: self.lifespan = 0\n",
    "        self.vy += GRAVITY\n",
    "        self.x += self.vx\n",
    "        self.y += self.vy\n",
    "\n",
    "    def draw(self, img):\n",
    "        if self.lifespan > 0:\n",
    "            alpha = self.lifespan / 255.0\n",
    "            draw_color = (int(self.color[0] * alpha), int(self.color[1] * alpha), int(self.color[2] * alpha))\n",
    "            cv2.circle(img, (int(self.x), int(self.y)), 2, draw_color, -1)\n",
    "\n",
    "    def is_dead(self):\n",
    "        return self.lifespan <= 0\n",
    "\n",
    "class Firework:\n",
    "    def __init__(self):\n",
    "        # 발사체는 항상 화면 하단에서 시작\n",
    "        self.x = random.randint(int(WIDTH*0.2), int(WIDTH*0.8))\n",
    "        self.y = HEIGHT\n",
    "        self.color = (random.randint(100, 255), random.randint(100, 255), random.randint(100, 255))\n",
    "        \n",
    "        # 위로 솟아오르는 속도\n",
    "        self.vx = random.uniform(-3, 3)\n",
    "        self.vy = random.uniform(-6, -8)\n",
    "        \n",
    "        self.exploded = False\n",
    "        self.particles = [] # 폭발 후 생성될 파티클 리스트\n",
    "\n",
    "    def update(self):\n",
    "        if not self.exploded:\n",
    "            # 발사체에 중력 적용 및 위치 업데이트\n",
    "            self.vy += GRAVITY\n",
    "            self.x += self.vx\n",
    "            self.y += self.vy\n",
    "            \n",
    "            # 최고점에 도달하면 (속도가 0 이상이 되면) 폭발\n",
    "            if self.vy >= 0:\n",
    "                self.explode()\n",
    "        else:\n",
    "            # 폭발 후에는 모든 파티클을 업데이트\n",
    "            for p in self.particles:\n",
    "                p.update()\n",
    "            # 수명이 다한 파티클은 리스트에서 제거\n",
    "            self.particles = [p for p in self.particles if not p.is_dead()]\n",
    "\n",
    "    def explode(self):\n",
    "        self.exploded = True\n",
    "        # 폭발 위치에서 여러 개의 최종 파티클을 생성\n",
    "        num_particles = random.randint(50, 100) # 더 화려하게 파티클 수를 늘림\n",
    "        for _ in range(num_particles):\n",
    "            self.particles.append(Particle(self.x, self.y, self.color))\n",
    "                \n",
    "    def draw(self, img):\n",
    "        if not self.exploded:\n",
    "            # 폭발 전에는 발사체(하나의 점)를 그림\n",
    "            cv2.circle(img, (int(self.x), int(self.y)), 3, self.color, -1)\n",
    "        else:\n",
    "            # 폭발 후에는 모든 파티클을 그림\n",
    "            for p in self.particles:\n",
    "                p.draw(img)\n",
    "\n",
    "    def is_done(self):\n",
    "        # 폭죽이 폭발했고, 모든 파티클이 사라졌으면 True 반환\n",
    "        return self.exploded and len(self.particles) == 0\n",
    "    \n",
    "\n",
    "def effect_firework(img):\n",
    "    # 폭죽 2개 생성\n",
    "    primary_fireworks = [Firework(), Firework()]\n",
    "\n",
    "    # 폭죽이 모두 끝날 때까지 루프 실행\n",
    "    while len(primary_fireworks) > 0:\n",
    "\n",
    "        ret, img = cap.read()\n",
    "\n",
    "        # 프레임 읽기 실패 시 루프 종료\n",
    "        if not ret:\n",
    "            print(\"프레임을 읽을 수 없습니다. 카메라 연결 또는 웹캠 상태를 확인하고 루프를 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        img = cv2.flip(img, 1) # 좌우 반전 (거울 모드)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # MediaPipe 처리를 위해 BGR -> RGB 변환\n",
    "        img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR) # OpenCV 표시를 위해 RGB -> BGR 변환\n",
    "        \n",
    "        # 폭죽 업데이트 및 그리기\n",
    "        for fw in primary_fireworks:\n",
    "            fw.update()\n",
    "            fw.draw(img)\n",
    "        \n",
    "        # 역할이 끝난 폭죽은 리스트에서 제거\n",
    "        primary_fireworks = [fw for fw in primary_fireworks if not fw.is_done()]\n",
    "        \n",
    "        cv2.imshow('Gesture Recognition (Two Hands)', img)\n",
    "        \n",
    "        # ESC 키를 누르면 즉시 종료\n",
    "        if cv2.waitKey(20) == 27:\n",
    "            break\n",
    "\n",
    "    # 루프가 끝나면 모든 창을 닫고 프로그램 종료\n",
    "    #cv2.destroyAllWindows()\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_cat(img, result):\n",
    "    # 손이 2개 미만으로 감지되면 원본 이미지 반환\n",
    "    if result.multi_hand_landmarks is None or len(result.multi_hand_landmarks) < 2:\n",
    "        return img\n",
    "\n",
    "    # 이미지 파일 로드 및 예외 처리\n",
    "    cat = cv2.imread('./img/cat.jpg')\n",
    "    if cat is None:\n",
    "        print(\"Error: 'cat.jpg' 이미지를 찾을 수 없습니다. 경로를 확인하세요.\")\n",
    "        return img\n",
    "\n",
    "    h_img, w_img, _ = img.shape\n",
    "\n",
    "    # 양손의 9번 랜드마크 좌표 추출\n",
    "    hand1 = result.multi_hand_landmarks[0].landmark[9]\n",
    "    hand2 = result.multi_hand_landmarks[1].landmark[9]\n",
    "\n",
    "    # 랜드마크 좌표를 이미지 픽셀 좌표로 변환\n",
    "    x1 = int(hand1.x * w_img)\n",
    "    y1 = int(hand1.y * h_img)\n",
    "    x2 = int(hand2.x * w_img)\n",
    "    y2 = int(hand2.y * h_img)\n",
    "\n",
    "    # 오버레이할 이미지의 너비와 위치 계산\n",
    "    w = abs(x1 - x2)  # 너비\n",
    "    x = min(x1, x2)   # 시작 x좌표\n",
    "    y = min(y1, y2)   # 시작 y좌표\n",
    "\n",
    "    # 너비가 0이면 그리지 않음\n",
    "    if w == 0:\n",
    "        return img\n",
    "\n",
    "    # 오버레이 이미지 리사이즈 및 마스크 생성\n",
    "    cat_ratio = (cat.shape[0] / cat.shape[1])\n",
    "    overlay_h = int(w * cat_ratio)\n",
    "    overlay_w = w\n",
    "    \n",
    "    if overlay_w <= 0 or overlay_h <= 0:\n",
    "        return img\n",
    "        \n",
    "    overlay_img = cv2.resize(cat, (overlay_w, overlay_h))\n",
    "    \n",
    "    # 흰색 배경을 제거\n",
    "    gray = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)\n",
    "    _, rm_bg = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # 이미지의 하단이 y좌표에 오도록 위치 계산\n",
    "    y_start = y - overlay_h\n",
    "    \n",
    "    # 원본 이미지 내에 그려질 영역(ROI) 계산\n",
    "    roi_y1 = max(y_start, 0)\n",
    "    roi_y2 = min(y_start + overlay_h, h_img)\n",
    "    roi_x1 = max(x, 0)\n",
    "    roi_x2 = min(x + overlay_w, w_img)\n",
    "\n",
    "    # ROI 영역을 원본 이미지에서 추출\n",
    "    roi = img[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "\n",
    "    # 오버레이 이미지와 마스크도 ROI 크기에 맞게 잘라내기\n",
    "    crop_y1 = roi_y1 - y_start\n",
    "    crop_x1 = roi_x1 - x\n",
    "    crop_y2 = crop_y1 + roi.shape[0]\n",
    "    crop_x2 = crop_x1 + roi.shape[1]\n",
    "\n",
    "    # 화면을 완전히 벗어난 경우\n",
    "    if roi.shape[0] <= 0 or roi.shape[1] <= 0:\n",
    "        return img\n",
    "\n",
    "    overlay_crop = overlay_img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "    mask_crop = rm_bg[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "    \n",
    "    if roi.shape != overlay_crop.shape:\n",
    "        return img\n",
    "\n",
    "    # 합성\n",
    "    bg_masked = cv2.bitwise_and(roi, roi, mask=cv2.bitwise_not(mask_crop))\n",
    "    fg_masked = cv2.bitwise_and(overlay_crop, overlay_crop, mask=mask_crop)\n",
    "    blended_roi = cv2.add(bg_masked, fg_masked)\n",
    "    \n",
    "    # 원본 이미지에 최종 결과물을 적용\n",
    "    img[roi_y1:roi_y2, roi_x1:roi_x2] = blended_roi\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_gun(img, pos):\n",
    "    # --- 효과 이미지 불러오기 ---\n",
    "    img_overlay = cv2.imread('./img/gun.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # gun 이미지 크기 확인 및 조정\n",
    "    if img_overlay is not None:\n",
    "        img_overlay = cv2.resize(img_overlay, (150, 150))  # 적절히 resize\n",
    "    else:\n",
    "        print(\"❌ gun 효과 이미지를 불러오지 못했습니다.\")\n",
    "        return img\n",
    "\n",
    "    x, y = pos\n",
    "    h, w = img_overlay.shape[:2]\n",
    "\n",
    "    if x + w > img.shape[1] or y + h > img.shape[0]:\n",
    "        return img\n",
    "\n",
    "    overlay_img = img_overlay[..., :3]\n",
    "    mask = img_overlay[..., 3:] / 255.0\n",
    "\n",
    "    roi = img[y:y+h, x:x+w]\n",
    "    img[y:y+h, x:x+w] = (1 - mask) * roi + mask * overlay_img\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_ont_the_cheek_gif = Image.open('./img/heart_on_the_cheek.gif')\n",
    "\n",
    "# 모든 프레임 추출\n",
    "heart_ont_the_cheek_frames = []\n",
    "try:\n",
    "    while True:\n",
    "        frame = heart_ont_the_cheek_gif.convert('RGBA')\n",
    "        frame_np = np.array(frame)\n",
    "        frame_cv = cv2.cvtColor(frame_np, cv2.COLOR_RGBA2BGRA)\n",
    "        heart_ont_the_cheek_frames.append(np.array(frame_cv))\n",
    "        heart_ont_the_cheek_gif.seek(heart_ont_the_cheek_gif.tell() + 1)\n",
    "except EOFError:\n",
    "    pass\n",
    "\n",
    "num_heart_ont_the_cheek_frames = len(heart_ont_the_cheek_frames)\n",
    "hc_frame_idx = 0\n",
    "\n",
    "# GIF 합성 함수 정의\n",
    "def effect_heart_ont_the_cheek(image, result, frame_idx):\n",
    "    hand1 = result.multi_hand_landmarks[0].landmark[9]\n",
    "    hand2 = result.multi_hand_landmarks[1].landmark[9]\n",
    "\n",
    "    hand1_x = int(hand1.x * image.shape[1])\n",
    "    hand2_x = int(hand2.x * image.shape[1])\n",
    "    hand1_y = int(hand1.y * image.shape[0])\n",
    "    hand2_y = int(hand2.y * image.shape[0])\n",
    "\n",
    "    # 1. 손 사이의 중심 좌표 계산\n",
    "    center_x = (hand1_x + hand2_x) // 2\n",
    "    center_y = (hand1_y + hand2_y) // 2\n",
    "\n",
    "    # 2. 두 손 사이 거리로 크기 결정 (너비 기준)\n",
    "    w = max(abs(hand1_x - hand2_x), abs(hand1_y - hand2_y))\n",
    "    if w == 0:\n",
    "        return image  # 거리 0이면 렌더링 안 함\n",
    "\n",
    "    # 3. 프레임 가져와서 크기 조정\n",
    "    overlay_img = heart_ont_the_cheek_frames[frame_idx % num_heart_ont_the_cheek_frames]\n",
    "    overlay_img = cv2.resize(overlay_img, (w, int(w * overlay_img.shape[0] / overlay_img.shape[1])))\n",
    "    h = overlay_img.shape[0]\n",
    "\n",
    "    # 4. 중앙 기준으로 오버레이 위치 계산\n",
    "    y1 = max(0, center_y - (h // 2))\n",
    "    y2 = min(image.shape[0], y1 + h)\n",
    "    x1 = max(0, center_x - (w // 2))\n",
    "    x2 = min(image.shape[1], x1 + w)\n",
    "\n",
    "    overlay_crop = overlay_img[0:(y2 - y1), 0:(x2 - x1), :]\n",
    "    mask_crop = overlay_crop[:, :, 3] / 255  # alpha mask\n",
    "\n",
    "    for c in range(3):\n",
    "        image[y1:y2, x1:x2, c] = \\\n",
    "            (overlay_crop[:, :, c] * mask_crop) + (image[y1:y2, x1:x2, c] * (1 - mask_crop))\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 실시간 제스처 추론 시작 ---\n",
      "   웹캠 창이 열리면 화면을 보면서 제스처를 수행하세요.\n",
      "   'q' 키를 누르면 언제든지 종료할 수 있습니다.\n",
      "카메라 스트림 시작...\n",
      "사용자 요청 ('q' 키)으로 추론을 중단합니다.\n",
      "\n",
      "--- 리소스 정리 중... ---\n",
      "   웹캠 리소스가 해제되었습니다.\n",
      "   모든 OpenCV 창이 닫혔습니다.\n",
      "   MediaPipe Hands 모델 리소스가 해제되었습니다.\n",
      "\n",
      "추론 스크립트 실행 완료.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Real-time Gesture Prediction Main Loop\n",
    "\n",
    "# --- 3.1 시퀀스 데이터 저장을 위한 버퍼 초기화 ---\n",
    "# LSTM 모델은 과거 'seq_length'만큼의 프레임 데이터를 필요로 합니다.\n",
    "# collections.deque는 고정된 최대 길이를 가지는 큐입니다.\n",
    "seq_data_buffer = deque(maxlen=seq_length)\n",
    "\n",
    "# --- 3.2 화면 표시 설정 ---\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_thickness = 2\n",
    "text_color_default = (255, 255, 255) # White\n",
    "\n",
    "print(\"\\n--- 실시간 제스처 추론 시작 ---\")\n",
    "print(\"   웹캠 창이 열리면 화면을 보면서 제스처를 수행하세요.\")\n",
    "print(\"   'q' 키를 누르면 언제든지 종료할 수 있습니다.\")\n",
    "\n",
    "try:\n",
    "    if not cap.isOpened() or model is None:\n",
    "        print(\"웹캠 또는 모델이 준비되지 않아 추론을 시작할 수 없습니다. 이전 셀의 오류를 확인하세요.\")\n",
    "    else:\n",
    "        print(\"카메라 스트림 시작...\")\n",
    "        # 무한 루프를 사용하여 웹캠 프레임을 계속 읽고 처리합니다.\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            # 프레임 읽기 실패 시 루프 종료\n",
    "            if not ret:\n",
    "                print(\"프레임을 읽을 수 없습니다. 카메라 연결 또는 웹캠 상태를 확인하고 루프를 종료합니다.\")\n",
    "                break\n",
    "\n",
    "            img = cv2.flip(img, 1) # 좌우 반전 (거울 모드)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # MediaPipe 처리를 위해 BGR -> RGB 변환\n",
    "            result = hands.process(img_rgb) # MediaPipe를 이용한 손 랜드마크 감지\n",
    "            img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR) # OpenCV 표시를 위해 RGB -> BGR 변환\n",
    "\n",
    "            # --- 3.3 현재 프레임의 특징 데이터 추출 및 패딩 ---\n",
    "            # 학습 시 사용했던 두 손 데이터를 구성하는 로직과 동일해야 합니다.\n",
    "            current_frame_features = []\n",
    "            num_detected_hands = 0\n",
    "            display_message = \"\"\n",
    "            current_text_color = text_color_default\n",
    "\n",
    "            if result.multi_hand_landmarks:\n",
    "                num_detected_hands = len(result.multi_hand_landmarks)\n",
    "                # ✨ 두 손이 모두 감지되었을 때만 실제 특징 데이터 처리\n",
    "                if num_detected_hands == 2:\n",
    "                    # 손의 x좌표를 기준으로 정렬하여 항상 일관된 순서(예: 왼손, 오른손)로 데이터를 구성\n",
    "                    handedness_sorted = [(lm.landmark[0].x, i) for i, lm in enumerate(result.multi_hand_landmarks)]\n",
    "                    handedness_sorted.sort()\n",
    "\n",
    "                    for _, hand_idx in handedness_sorted:\n",
    "                        res = result.multi_hand_landmarks[hand_idx]\n",
    "\n",
    "                        # 랜드마크 추출 (21개 관절 * 4차원 = 84개 특징)\n",
    "                        joint = np.zeros((21, 4))\n",
    "                        for j, lm in enumerate(res.landmark):\n",
    "                            joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "                        current_frame_features.extend(joint.flatten().tolist())\n",
    "\n",
    "                        # 관절 각도 계산 (15개 특징)\n",
    "                        v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "                        v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "                        v = v2 - v1\n",
    "                        norm_v = np.linalg.norm(v, axis=1)\n",
    "                        v = v / (norm_v[:, np.newaxis] + 1e-8) # 0으로 나누는 오류 방지\n",
    "                        dot_product = np.einsum('nt,nt->n',\n",
    "                                                 v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],\n",
    "                                                 v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])\n",
    "                        dot_product = np.clip(dot_product, -1.0, 1.0) # arccos 도메인 유지\n",
    "                        angle = np.degrees(np.arccos(dot_product))\n",
    "                        current_frame_features.extend(angle.tolist())\n",
    "\n",
    "                        #mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS) # 화면에 랜드마크 그리기\n",
    "\n",
    "                    # 두 손의 특징 데이터가 모두 추출되었는지 최종 확인 (총 198개)\n",
    "                    expected_features_len = (21 * 4 + 15) * 2\n",
    "                    if len(current_frame_features) != expected_features_len:\n",
    "                        print(f\"특징 개수 불일치! (예상: {expected_features_len}, 실제: {len(current_frame_features)})\")\n",
    "                        # 불일치 시 0으로 패딩하여 모델 입력 shape 유지\n",
    "                        current_frame_features = np.zeros(expected_features_len, dtype=np.float32).tolist()\n",
    "                        display_message = \"Data Error - Padding\"\n",
    "                        current_text_color = (0, 0, 255) # Red\n",
    "                else:\n",
    "                    # 손이 감지되었지만 2개가 아닌 경우 (1개 또는 3개 이상)\n",
    "                    expected_features_len = (21 * 4 + 15) * 2 # 항상 198개\n",
    "                    current_frame_features = np.zeros(expected_features_len, dtype=np.float32).tolist()\n",
    "                    display_message = f\"Detected {num_detected_hands} hand(s). Need 2.\"\n",
    "                    current_text_color = (0, 165, 255) # Orange\n",
    "            else:\n",
    "                # 손이 전혀 감지되지 않은 경우\n",
    "                expected_features_len = (21 * 4 + 15) * 2 # 항상 198개\n",
    "                current_frame_features = np.zeros(expected_features_len, dtype=np.float32).tolist()\n",
    "                display_message = \"No hands detected.\"\n",
    "                current_text_color = (0, 0, 255) # Red\n",
    "\n",
    "            # 추출된 (또는 패딩된) 특징 데이터를 시퀀스 버퍼에 추가\n",
    "            seq_data_buffer.append(np.array(current_frame_features, dtype=np.float32))\n",
    "\n",
    "\n",
    "            # --- 3.4 시퀀스 버퍼가 충분히 채워지면 예측 수행 ---\n",
    "            if len(seq_data_buffer) == seq_length:\n",
    "                # 모델 입력 형태에 맞게 차원 확장 (배치 차원 추가)\n",
    "                input_sequence = np.expand_dims(np.array(seq_data_buffer), axis=0) # (1, seq_length, num_features)\n",
    "\n",
    "                if model is not None: # 모델이 성공적으로 로드되었는지 확인\n",
    "                    preds = model.predict(input_sequence, verbose=0)[0] # verbose=0으로 예측 진행바 숨기기\n",
    "                    action_idx = np.argmax(preds)\n",
    "                    confidence = preds[action_idx]\n",
    "\n",
    "                    # 신뢰도가 높은 예측만 표시 ( threshold 조절 가능)\n",
    "                    if confidence > 0.7: # 70% 이상의 신뢰도일 때만 제스처 이름 표시\n",
    "                        predicted_action = actions[action_idx]\n",
    "                        display_message = f\"Action: {predicted_action} ({confidence:.2f})\"\n",
    "                        current_text_color = (0, 255, 0) # Green\n",
    "\n",
    "                        # 이펙트 적용\n",
    "                        if predicted_action == \"flower\":\n",
    "                            img = effect_flower(img, result, frame_idx)\n",
    "                            frame_idx += 1\n",
    "                        elif predicted_action == \"crown\":\n",
    "                            img = effect_crown(img, result)\n",
    "                        elif predicted_action == \"heart_beat\":\n",
    "                            img = effect_heartbeat(img, result, hb_frame_idx)\n",
    "                            hb_frame_idx += 1\n",
    "                        elif predicted_action == \"firework\":\n",
    "                            img = effect_firework(img)\n",
    "                            seq_data_buffer = deque(maxlen=seq_length) \n",
    "                        #elif predicted_action == \"bear\":\n",
    "                            \n",
    "                        elif predicted_action == \"cat\":\n",
    "                            img = effect_cat(img, result)\n",
    "                        #elif predicted_action == \"son_celebration\":\n",
    "                            \n",
    "                        elif predicted_action == \"heart_on_the_cheek\":\n",
    "                            img = effect_heart_ont_the_cheek(img, result, hc_frame_idx)\n",
    "                            hc_frame_idx += 1\n",
    "                        elif predicted_action == \"gun\":\n",
    "                            img = effect_gun(img, pos=(320, 240))\n",
    "                        #elif predicted_action == \"pipe\":\n",
    "                            \n",
    "                        elif predicted_action == \"tiger\":\n",
    "                            effect_tiger(img, result)\n",
    "                            #seq_data_buffer.clear()\n",
    "                        #else:\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        display_message = \"Action: Thinking...\" # 신뢰도가 낮으면 \"생각 중\"으로 표시\n",
    "                        current_text_color = (0, 165, 255) # Orange\n",
    "                else:\n",
    "                    display_message = \"Model not loaded.\" # 모델이 로드되지 않았을 경우\n",
    "                    current_text_color = (0, 0, 255) # Red\n",
    "            else:\n",
    "                # 버퍼가 채워지는 동안 메시지 표시\n",
    "                display_message = f\"Buffering... ({len(seq_data_buffer)}/{seq_length})\"\n",
    "                current_text_color = (255, 255, 0) # Yellow\n",
    "\n",
    "\n",
    "            # --- 3.5 웹캠 화면에 정보 표시 및 업데이트 ---\n",
    "            cv2.putText(img, display_message, (10, 60), font, font_scale, current_text_color, font_thickness)\n",
    "            cv2.imshow('Gesture Recognition (Two Hands)', img)\n",
    "\n",
    "            # --- 3.6 종료 조건 확인 ('q' 키) ---\n",
    "            key = cv2.waitKey(1) & 0xFF # 1ms 대기 및 키 입력 확인\n",
    "            if key == ord('q'):\n",
    "                print(\"사용자 요청 ('q' 키)으로 추론을 중단합니다.\")\n",
    "                break\n",
    "            # 참고: 창 닫기 버튼으로 종료하는 기능은 Jupyter 환경에서 불안정할 수 있습니다.\n",
    "            # `if cv2.getWindowProperty('Gesture Recognition (Two Hands)', cv2.WND_PROP_VISIBLE) < 1:` 와 같은 코드는\n",
    "            # 특정 환경에서 오류를 유발할 수 있어 일반적으로 'q' 키를 권장합니다.\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- 추론 중 치명적인 오류 발생 ---\")\n",
    "    print(f\"   오류 내용: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc() # 상세한 에러 스택 트레이스 출력 (디버깅에 유용)\n",
    "\n",
    "finally:\n",
    "    # --- 3.7 리소스 정리 (예외 발생 여부와 상관없이 항상 실행) ---\n",
    "    print(\"\\n--- 리소스 정리 중... ---\")\n",
    "    if 'cap' in locals() and cap.isOpened():\n",
    "        cap.release()\n",
    "        print(\"   웹캠 리소스가 해제되었습니다.\")\n",
    "    else:\n",
    "        print(\"   웹캠이 이미 닫혀 있거나 초기화되지 않았습니다.\")\n",
    "\n",
    "    cv2.destroyAllWindows() # 모든 OpenCV 창 닫기\n",
    "    print(\"   모든 OpenCV 창이 닫혔습니다.\")\n",
    "\n",
    "    if 'hands' in locals():\n",
    "        hands.close() # MediaPipe Hands 모델 리소스 해제\n",
    "        print(\"   MediaPipe Hands 모델 리소스가 해제되었습니다.\")\n",
    "\n",
    "print(\"\\n추론 스크립트 실행 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniaiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
